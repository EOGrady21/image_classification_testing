{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sbianco78/unsupervisedPlanktonLearning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPiFEepnkQU17X0s81hjiHq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Echisholm21/image_classification_testing/blob/master/sbianco78_unsupervisedPlanktonLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQNGBjuEOtGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import section\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from skimage import feature\n",
        "from itertools import cycle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras import Model\n",
        "from scipy import interp\n",
        "import random\n",
        "from mahotas.zernike import zernike_moments\n",
        "from mahotas.features import haralick\n",
        "from keras.optimizers import Adam\n",
        "import cv2\n",
        "import keras\n",
        "import copy\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.callbacks import Callback\n",
        "from keras.layers import Lambda\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l1\n",
        "import os\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Dropout, Activation, concatenate\n",
        "import numpy as np\n",
        "from sklearn_extensions.fuzzy_kmeans import KMedians, FuzzyKMeans, KMeans\n",
        "\n",
        "\n",
        "class PLANKTON_CLASSIFIER():\n",
        "\n",
        "    def normalize_test_train_for_newclasses(self, features, x_test, name):\n",
        "\n",
        "        medie_per_norm = np.loadtxt(self.address + 'TRAINED DETECTORS/medie' + name + '.csv')\n",
        "        medie_per_std = np.loadtxt(self.address + 'TRAINED DETECTORS/medie1' + name + '.csv')\n",
        "\n",
        "        for i in range(0, features):\n",
        "            x_test[:, i] = (x_test[:, i] - medie_per_std[i]) / (-medie_per_std[i] + medie_per_norm[i])\n",
        "\n",
        "        return x_test\n",
        "\n",
        "    class LocalBinaryPatterns:\n",
        "        def __init__(self, numPoints, radius):\n",
        "            # store the number of points and radius\n",
        "            self.numPoints = numPoints\n",
        "            self.radius = radius\n",
        "\n",
        "\n",
        "\n",
        "        def describe(self, image, eps=1e-7):\n",
        "            # compute the Local Binary Pattern representation\n",
        "            # of the image, and then use the LBP representation\n",
        "            # to build the histogram of patterns\n",
        "            lbp = feature.local_binary_pattern(image, self.numPoints,\n",
        "                                               self.radius, method=\"uniform\")\n",
        "            (hist, _) = np.histogram(lbp.ravel(),\n",
        "                                     bins=np.arange(0, self.numPoints + 3),\n",
        "                                     range=(0, self.numPoints + 2))\n",
        "\n",
        "            # normalize the histogram\n",
        "            hist = hist.astype(\"float\")\n",
        "            hist /= (hist.sum() + eps)\n",
        "\n",
        "            # return the histogram of Local Binary Patterns\n",
        "            return hist\n",
        "\n",
        "    def adjust_gamma(self, image, gamma=1.0):\n",
        "        # build a lookup table mapping the pixel values [0, 255] to\n",
        "        # their adjusted gamma values\n",
        "        invGamma = 1.0 / gamma\n",
        "        table = np.array([((l / 255.0) ** invGamma) * 255\n",
        "                          for l in np.arange(0, 256)]).astype(\"uint8\")\n",
        "\n",
        "        # apply gamma correction using the lookup table\n",
        "        return cv2.LUT(image, table)\n",
        "\n",
        "    # put here the address.\n",
        "    def image_processor(self, address):\n",
        "\n",
        "        train_folder = address + 'TRAIN_IMAGE/'\n",
        "        test_folder = address + 'TEST_IMAGE/'\n",
        "        output_segmentation_train = address + 'BIN_TRAIN_IMAGE/'\n",
        "        output_segmentation_test = address + 'BIN_TEST_IMAGE/'\n",
        "\n",
        "        if not os.path.isdir(output_segmentation_test):\n",
        "            os.mkdir(output_segmentation_test)\n",
        "\n",
        "        if not os.path.isdir(output_segmentation_train):\n",
        "            os.mkdir(output_segmentation_train)\n",
        "\n",
        "        files = self.files\n",
        "\n",
        "        mydir2 = test_folder\n",
        "\n",
        "        for j in range(0, len(files)):\n",
        "\n",
        "            if files[j] != \".DS_Store\":\n",
        "                if not os.path.isdir(output_segmentation_train + files[j]):\n",
        "                    os.mkdir(output_segmentation_train + files[j])\n",
        "\n",
        "                files2 = os.listdir(train_folder + '/' + files[j])\n",
        "\n",
        "                a44 = 100\n",
        "                if len(files2) <= a44:\n",
        "                    a44 = len(files2)\n",
        "                for aa in range(0, a44):\n",
        "\n",
        "                    if files2[aa] != \".DS_Store\":\n",
        "\n",
        "                        img = cv2.imread(train_folder + '/' + files[j] + '/' + files2[aa])\n",
        "                        img = cv2.medianBlur(img, 7, img)\n",
        "                        #\n",
        "                        try:\n",
        "\n",
        "                            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                            img2 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,\n",
        "                                                         15, 2)\n",
        "\n",
        "                            img2 = cv2.bitwise_not(img2)\n",
        "                            img2 = cv2.morphologyEx(img2, cv2.MORPH_CLOSE,\n",
        "                                                    cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))\n",
        "\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                        try:\n",
        "                            img = cv2.imread(train_folder + '/' + files[j] + '/' + files2[aa])\n",
        "                            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                            ret2, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_TRIANGLE)\n",
        "                            img = cv2.bitwise_not(img)\n",
        "                            img = cv2.morphologyEx(img, cv2.MORPH_CLOSE,\n",
        "                                                   cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))\n",
        "\n",
        "                            img = cv2.convertScaleAbs(img)\n",
        "                            im3 = copy.copy(img)\n",
        "                            img = cv2.bitwise_or(img, img2)\n",
        "                            # edge detection\n",
        "\n",
        "                            im2, x, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "                            z = 0\n",
        "                            frame4 = img\n",
        "                            imy = np.zeros_like(im3)\n",
        "                            ar_max = 0\n",
        "                            ar_max2 = 0\n",
        "                            for i in x:\n",
        "                                # minimum rectangle containing the object\n",
        "                                PO2 = cv2.boundingRect(i)\n",
        "                                area = cv2.countNonZero(frame4[PO2[1]:(PO2[1] + PO2[3]), PO2[0]:(PO2[0] + PO2[2])])\n",
        "                                # LET'S CHOOSE THE AREA THAT WE WANT\n",
        "                                if area > 200 and area < np.shape(img)[0] * np.shape(img)[1]:\n",
        "\n",
        "                                    moments = cv2.moments(i)\n",
        "                                    cv2.drawContours(imy, [i], -1, (255, 0, 0), -1)\n",
        "\n",
        "                                    ar = moments['m00']\n",
        "                                    if ar > ar_max2:\n",
        "                                        ar_max = i\n",
        "                                        ar_max2 = ar\n",
        "\n",
        "                            try:\n",
        "\n",
        "                                i = ar_max\n",
        "                                PO2 = cv2.boundingRect(i)\n",
        "                                imx = np.zeros_like(im3)\n",
        "                                cv2.drawContours(imx, [i], -1, (255, 0, 0), -1)\n",
        "                                o = imx\n",
        "                                ttemp = copy.copy(im3)\n",
        "                                ttemp[ttemp != 0] = 1\n",
        "                                o[o != 0] = 1\n",
        "                                imy[imy != 0] = 1\n",
        "                                if np.sum(o) < np.sum(imy) * 0.5:\n",
        "                                    ############################################\n",
        "                                    img = cv2.morphologyEx(img, cv2.MORPH_CLOSE,\n",
        "                                                           cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)))\n",
        "                                    im2, x, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "                                    z = 0\n",
        "                                    frame4 = img\n",
        "\n",
        "                                    ar_max = 0\n",
        "                                    ar_max2 = 0\n",
        "                                    for i in x:\n",
        "                                        # minimum rectangle containing the object\n",
        "                                        PO2 = cv2.boundingRect(i)\n",
        "                                        area = cv2.countNonZero(\n",
        "                                            frame4[PO2[1]:(PO2[1] + PO2[3]), PO2[0]:(PO2[0] + PO2[2])])\n",
        "                                        # LET'S CHOOSE THE AREA THAT WE WANT\n",
        "                                        if area > 200 and area < np.shape(img)[0] * np.shape(img)[1]:\n",
        "\n",
        "                                            moments = cv2.moments(i)\n",
        "                                            ar = moments['m00']\n",
        "                                            if ar > ar_max2:\n",
        "                                                ar_max = i\n",
        "                                                ar_max2 = ar\n",
        "\n",
        "                                    try:\n",
        "                                        i = ar_max\n",
        "                                        PO2 = cv2.boundingRect(i)\n",
        "                                        optim_contour = np.zeros_like(im3)\n",
        "                                        area = cv2.countNonZero(\n",
        "                                            frame4[PO2[1]:(PO2[1] + PO2[3]), PO2[0]:(PO2[0] + PO2[2])])\n",
        "                                        cv2.drawContours(optim_contour, [i], -1, (255, 255, 0), -1)\n",
        "                                        #######################display objects###########################ll\n",
        "                                        if np.sum(optim_contour) > 255 * np.shape(optim_contour)[1] * \\\n",
        "                                                np.shape(optim_contour)[0] - 500:\n",
        "                                            continue\n",
        "                                        cv2.imwrite(output_segmentation_train + '/' + files[j] + '/' + files2[aa],\n",
        "                                                    optim_contour)\n",
        "                                        continue\n",
        "                                    except:\n",
        "                                        pass\n",
        "\n",
        "                                ############################################\n",
        "\n",
        "                                area = cv2.countNonZero(frame4[PO2[1]:(PO2[1] + PO2[3]), PO2[0]:(PO2[0] + PO2[2])])\n",
        "                                optim_contour = np.zeros_like(im3)\n",
        "                                area = cv2.countNonZero(frame4[PO2[1]:(PO2[1] + PO2[3]), PO2[0]:(PO2[0] + PO2[2])])\n",
        "                                cv2.drawContours(optim_contour, [i], -1, (255, 0, 255), -1)\n",
        "\n",
        "                                #######################display objects###########################ll\n",
        "\n",
        "                                if np.sum(optim_contour) > 255 * np.shape(optim_contour)[1] * np.shape(optim_contour)[\n",
        "                                    0] - 500:\n",
        "                                    continue\n",
        "\n",
        "                                cv2.imwrite(output_segmentation_train + '/' + files[j] + '/' + files2[aa],\n",
        "                                            optim_contour)\n",
        "                            except:\n",
        "                                pass\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "    def feature_extractor(self, address, train_folder, train_folder_bin, files):\n",
        "\n",
        "        train_folder = address + 'TRAIN_IMAGE/'\n",
        "        train_folder_bin = address + 'BIN_TRAIN_IMAGE/'\n",
        "        train_features_bin_OUTPUT = address + 'TRAIN_FEATURES/'\n",
        "\n",
        "        if not os.path.isdir(train_features_bin_OUTPUT):\n",
        "            os.mkdir(train_features_bin_OUTPUT)\n",
        "\n",
        "        total = []\n",
        "\n",
        "        count = np.zeros((10))\n",
        "\n",
        "        for j in range(0, len(files)):\n",
        "            if files[j] != \".DS_Store\":\n",
        "                conteggio = 0\n",
        "\n",
        "                if not os.path.isdir(train_features_bin_OUTPUT + files[j]):\n",
        "                    os.mkdir(train_features_bin_OUTPUT + files[j])\n",
        "\n",
        "                files2 = os.listdir(train_folder_bin + files[j])\n",
        "                # files2 = total[np.where( files44 == files[j])[0][0]]\n",
        "\n",
        "                for aa in range(0, len(files2)):\n",
        "\n",
        "                    if files2[aa] != \".DS_Store\":\n",
        "\n",
        "                        img = cv2.imread(train_folder_bin + '\\\\' + files[j] + '\\\\' + files2[aa])\n",
        "\n",
        "                        try:\n",
        "                            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                        except:\n",
        "                            continue\n",
        "                            pass\n",
        "\n",
        "                        im3 = img\n",
        "\n",
        "                        # ZERNIKE MOMENTS\n",
        "\n",
        "                        list = np.zeros((133, 1), dtype=float)\n",
        "\n",
        "                        W, H = im3.shape\n",
        "                        R = min(W, H) / 2\n",
        "                        a = zernike_moments(im3, R, 8)\n",
        "                        try:\n",
        "\n",
        "                            im5 = cv2.imread(train_folder + '/' + files[j] + '/' + files2[aa])\n",
        "                            im4 = copy.copy(im5)\n",
        "\n",
        "                            if np.shape(im5)[0] != np.shape(im3)[0]:\n",
        "                                deltax = np.shape(im5)[0] - np.shape(im3)[0]\n",
        "\n",
        "                                if deltax < 0:\n",
        "                                    im5 = np.pad(im5, pad_width=((-deltax, 0), (0, 0)), mode='constant')\n",
        "\n",
        "                                if deltax > 0:\n",
        "                                    im3 = np.pad(im3, pad_width=((deltax, 0), (0, 0)), mode='constant')\n",
        "\n",
        "                            if np.shape(im5)[1] != np.shape(im3)[1]:\n",
        "\n",
        "                                deltay = np.shape(im5)[1] - np.shape(im3)[1]\n",
        "\n",
        "                                if deltay > 0:\n",
        "                                    im3 = np.pad(im3, pad_width=((0, 0), (0, deltay)), mode='constant')\n",
        "\n",
        "                                if deltay < 0:\n",
        "                                    im5 = np.pad(im5, pad_width=((0, 0), (0, -deltay)), mode='constant')\n",
        "\n",
        "                            mask = np.zeros(im3.shape[:2], np.uint8)\n",
        "\n",
        "                            mask[im3 == 0] = 255\n",
        "\n",
        "                        except:\n",
        "                            continue\n",
        "\n",
        "                        try:\n",
        "\n",
        "                            momentsintensity = np.zeros((8, 1))\n",
        "                            imgray = copy.copy(im5)\n",
        "                            imgray = cv2.cvtColor(imgray, cv2.COLOR_BGR2GRAY)\n",
        "                            histogram = cv2.calcHist([imgray], [0], mask, [255], [0, 255])\n",
        "\n",
        "                            momentsintensity[0] = histogram.mean()\n",
        "                            momentsintensity[1] = histogram.std()\n",
        "                            from scipy.stats import kurtosis\n",
        "                            from scipy.stats import skew\n",
        "\n",
        "                            momentsintensity[2] = skew(histogram)\n",
        "                            momentsintensity[3] = kurtosis(histogram)\n",
        "                            from scipy.stats import entropy\n",
        "\n",
        "                            histogram = histogram / np.max(histogram)\n",
        "                            momentsintensity[4] = entropy(histogram)\n",
        "\n",
        "                            uuu = copy.copy(im5)\n",
        "\n",
        "                            # uuu[:, :, 0] = cv2.bitwise_and(uuu[:, :, 0], mask)\n",
        "                            # uuu[:, :, 1] = cv2.bitwise_and(uuu[:, :, 1], mask)\n",
        "                            # uuu[:, :, 2] = cv2.bitwise_and(uuu[:, :, 2], mask)\n",
        "                            #\n",
        "                            # momentsintensity[5] = np.mean(uuu[:, :, 0]) / np.mean(uuu[:, :, 2])\n",
        "                            # momentsintensity[6] = np.mean(uuu[:, :, 0]) / np.mean(uuu[:, :, 1])\n",
        "                            # momentsintensity[7] = np.mean(uuu[:, :, 1]) / np.mean(uuu[:, :, 2])\n",
        "                            im5[im3 == 0] = 255\n",
        "                            im5 = cv2.cvtColor(im5, cv2.COLOR_BGR2GRAY)\n",
        "                            im4 = copy.copy(im5)\n",
        "                            # im4 = cv2.medianBlur(im5, 3, dst=im4)\n",
        "                            # SHAPE INDEX\n",
        "\n",
        "\n",
        "\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                        try:\n",
        "                            import matplotlib.pyplot as plt\n",
        "                            from mpl_toolkits.mplot3d import Axes3D\n",
        "                            from scipy import ndimage as ndi\n",
        "                            from skimage.feature import shape_index\n",
        "                            from skimage.draw import circle\n",
        "\n",
        "                            # LOCAL BINARY PATTERNS\n",
        "\n",
        "                            lbp = self.LocalBinaryPatterns(54, 8)\n",
        "                            lbp = lbp.describe(im5)\n",
        "                            # HARALICK FEATURES\n",
        "                            b = haralick(im5)\n",
        "                            b = b.mean(axis=0)\n",
        "                            cont = 0\n",
        "                            # edge_detection\n",
        "                            im2, x, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "                            # ready for the list to save\n",
        "                            z = 0\n",
        "                            frame4 = img\n",
        "                            ar_max = 0\n",
        "                            ar_max2 = 0\n",
        "\n",
        "                            for i in x:\n",
        "                                # minimum rectangle containing the object\n",
        "                                PO2 = cv2.boundingRect(i)\n",
        "                                area = cv2.countNonZero(frame4[PO2[1]:(PO2[1] + PO2[3]), PO2[0]:(PO2[0] + PO2[2])])\n",
        "                                # LET'S CHOOSE THE AREA THAT WE WANT\n",
        "                                if area > 200 and area < np.shape(img)[0] * np.shape(img)[1]:\n",
        "\n",
        "                                    moments = cv2.moments(i)\n",
        "\n",
        "                                    ar = moments['m00']\n",
        "                                    if ar > ar_max2:\n",
        "                                        ar_max = i\n",
        "                                        ar_max2 = ar\n",
        "\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                        try:\n",
        "\n",
        "                            i = ar_max\n",
        "                            # minimum rectangle containing the object\n",
        "                            PO2 = cv2.boundingRect(i)\n",
        "                            area = cv2.countNonZero(frame4[PO2[1]:(PO2[1] + PO2[3]), PO2[0]:(PO2[0] + PO2[2])])\n",
        "                            # area must be computed in this way, many times contour can be opened.\n",
        "                            PO = PO2\n",
        "\n",
        "                            # LET'S CHOOSE THE AREA THAT WE WANT\n",
        "                            if area > 200 and area < np.shape(img)[0] * np.shape(img)[1]:\n",
        "\n",
        "                                # extract some features\n",
        "                                moments = cv2.moments(i)\n",
        "                                if moments['m00'] != 0.0:\n",
        "                                    #     # centroid\n",
        "                                    xc = moments['m10'] / moments['m00']\n",
        "\n",
        "                                    yc = moments['m01'] / moments['m00']\n",
        "                                # cont += 1\n",
        "                                humoments = cv2.HuMoments(moments)\n",
        "\n",
        "                                list[cont, 0] = moments['m00']\n",
        "                                cont += 1\n",
        "                                list[cont, 0] = humoments[0]\n",
        "                                cont += 1\n",
        "                                list[cont, 0] = humoments[1]\n",
        "                                cont += 1\n",
        "                                list[cont, 0] = humoments[2]\n",
        "                                cont += 1\n",
        "                                list[cont, 0] = humoments[3]\n",
        "                                cont += 1\n",
        "                                list[cont, 0] = humoments[4]\n",
        "                                cont += 1\n",
        "                                list[cont, 0] = humoments[5]\n",
        "                                cont += 1\n",
        "                                list[cont, 0] = humoments[6]\n",
        "                                cont += 1\n",
        "                                list[cont, 0] = area\n",
        "                                cont += 1\n",
        "                                # perimeter\n",
        "                                list[cont, 0] = cv2.arcLength(i, True)\n",
        "                                cont += 1\n",
        "                                # equivalent diameter\n",
        "                                list[cont, 0] = np.sqrt(4 * list[0] / np.pi)\n",
        "                                cont += 1\n",
        "                                ellip = cv2.fitEllipse(i)\n",
        "                                (center, axes, orientation) = ellip\n",
        "                                minor_axis = min(axes)\n",
        "                                major_axis = max(axes)\n",
        "                                list[cont, 0] = np.sqrt(1 - (minor_axis * minor_axis) / (major_axis * major_axis))\n",
        "                                # length of MAJOR and minor axis of fitting ellipse\n",
        "                                cont += 1\n",
        "                                list[cont, 0] = minor_axis\n",
        "                                cont += 1\n",
        "                                list[cont, 0] = major_axis\n",
        "                                cont += 1\n",
        "                                # roundness\n",
        "                                list[cont, 0] = (4 * np.pi * list[0]) / (\n",
        "                                        cv2.arcLength(i, True) * cv2.arcLength(i, True))\n",
        "                                cont += 1\n",
        "                                # color information\n",
        "                                width = PO[3]\n",
        "                                height = PO[2]\n",
        "                                # shape factor as percentace of occupancy\n",
        "                                list[cont, 0] = area / (width * height)\n",
        "                                cont += 1\n",
        "                                list[cont, 0] = (moments['m00'] * moments['m00']) / (\n",
        "                                        2 * np.pi * (moments['mu20'] + moments['mu02']))\n",
        "                                cont += 1\n",
        "                                rect = cv2.minAreaRect(i)\n",
        "                                box = cv2.boxPoints(rect)\n",
        "                                x1 = box[0, 0]\n",
        "                                y1 = box[0, 1]\n",
        "                                x2 = box[1, 0]\n",
        "                                y2 = box[1, 1]\n",
        "                                x3 = box[2, 0]\n",
        "                                y3 = box[2, 1]\n",
        "                                tocontinue = 0\n",
        "                                # rectangluar\n",
        "                                try:\n",
        "                                    list[cont, 0] = moments['m00'] / (\n",
        "                                            np.abs((rect[1][0] - rect[0][0])) * np.abs((rect[1][1] - rect[0][1])))\n",
        "                                    if np.abs((rect[1][0] - rect[0][0])) * np.abs((rect[1][1] - rect[0][1])) == 0:\n",
        "                                        tocontinue = 1\n",
        "                                    cont += 1\n",
        "\n",
        "                                    convex = cv2.convexHull(i)\n",
        "                                    list[cont, 0] = cv2.arcLength(i, True) / cv2.arcLength(convex, True)\n",
        "                                    cont += 1\n",
        "                                    if cv2.arcLength(convex, True) == 0:\n",
        "                                        tocontinue = 1\n",
        "\n",
        "                                except:\n",
        "                                    pass\n",
        "\n",
        "                                if tocontinue == 1:\n",
        "                                    continue\n",
        "                                # solidity\n",
        "                                widthmin = np.sqrt((x2 - x1) * (x2 - x1) + (y2 - y1) * (y2 - y1))\n",
        "                                heightmin = np.sqrt((x3 - x2) * (x3 - x2) + (y3 - y2) * (y3 - y2))\n",
        "                                if widthmin > heightmin:\n",
        "                                    list[cont, 0] = widthmin / heightmin\n",
        "                                    cont += 1\n",
        "                                    # higher dimension\n",
        "                                    list[cont, 0] = widthmin\n",
        "                                    cont += 1\n",
        "                                    # lower dinension\n",
        "                                    list[cont, 0] = heightmin\n",
        "                                    cont += 1\n",
        "                                else:\n",
        "                                    list[cont, 0] = heightmin / widthmin\n",
        "                                    cont += 1\n",
        "                                    # higher dimension\n",
        "                                    list[cont, 0] = heightmin\n",
        "                                    cont += 1\n",
        "                                    # lower dinension\n",
        "                                    list[cont, 0] = widthmin\n",
        "                                    cont += 1\n",
        "                                for newindex in range(2, len(a)):\n",
        "                                    list[cont, 0] = a[newindex]\n",
        "                                    cont += 1\n",
        "                                for newindex in range(0, len(lbp)):\n",
        "                                    list[cont, 0] = lbp[newindex]\n",
        "                                    cont += 1\n",
        "                                for newindex in range(0, len(b)):\n",
        "                                    list[cont, 0] = b[newindex]\n",
        "                                    cont += 1\n",
        "                                for newindex in range(0, len(momentsintensity)):\n",
        "                                    list[cont, 0] = momentsintensity[newindex]\n",
        "                                    cont += 1\n",
        "                                points = i\n",
        "                                s = np.zeros((len(points), 1), dtype=np.complex64)\n",
        "                                count = 0\n",
        "\n",
        "                                xc2 = 1 / len(points) * np.sum(points[:, 0, 0])\n",
        "\n",
        "                                yc2 = 1 / len(points) * np.sum(points[:, 0, 1])\n",
        "                                for p in points:\n",
        "                                    # s[count, 0] = ((p[0][0] + (p[0][1]*1j)))\n",
        "                                    s[count, 0] = ((p[0][0] - xc2) ** 2 + (p[0][1] - yc2) ** 2) ** 0.5\n",
        "                                    count += 1\n",
        "                                fourier = np.fft.fft(s)\n",
        "                                rho = np.abs(fourier)\n",
        "                                rho = rho / rho[0]\n",
        "                                rho = rho[0:11]\n",
        "                                medd = np.mean(s)\n",
        "                                stddd = np.std(s)\n",
        "                                AAA = np.fft.ifft(fourier)\n",
        "                                plt.plot(AAA)\n",
        "                                for newindex in range(0, len(rho)):\n",
        "                                    list[cont, 0] = rho[newindex]\n",
        "                                    cont += 1\n",
        "                                if conteggio < 1500:\n",
        "                                    np.savetxt(\n",
        "                                        train_features_bin_OUTPUT + '\\\\' + files[j] + '\\\\' + files2[conteggio] + '.csv',\n",
        "                                        list)\n",
        "                                    conteggio += 1\n",
        "\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "    def euclidian_distance(self, a, b):\n",
        "        c = b\n",
        "        for i in range(0, np.shape(a)[0] - 1):\n",
        "            c = np.row_stack((c, b))\n",
        "        dist = np.linalg.norm(a - c, axis=1)\n",
        "        return dist\n",
        "\n",
        "    # function needed to normalize the test set in the same way than the training, using training max and min value\n",
        "    def normalize_test_train(self, features, X, x_test):\n",
        "\n",
        "        medie_per_norm = np.zeros((features, 1), dtype=np.float)\n",
        "        medie_per_std = np.zeros((features, 1), dtype=np.float)\n",
        "\n",
        "        for i in range(0, features):\n",
        "            medie_per_norm[i] = float(np.max(X[:, i]))\n",
        "            medie_per_std[i] = float(np.min(X[:, i]))\n",
        "\n",
        "            # train normalization\n",
        "            X[:, i] = (X[:, i] - medie_per_std[i]) / (-medie_per_std[i] + medie_per_norm[i])\n",
        "\n",
        "        for i in range(0, features):\n",
        "            # test normalization\n",
        "            x_test[:, i] = (x_test[:, i] - medie_per_std[i]) / (-medie_per_std[i] + medie_per_norm[i])\n",
        "\n",
        "        # let us save these values for further analysis\n",
        "\n",
        "        return X, x_test\n",
        "\n",
        "    # we use this normalization for preprocessing and PCA analysis\n",
        "    def normalize(self, X):\n",
        "\n",
        "        for i in range(0, np.shape(X)[1]):\n",
        "            if np.sum(X[:, i]) != 0 and np.sum(X[:, i]) != (np.shape(X[:, i])[0] * X[0, i]):\n",
        "                X[:, i] = (X[:, i] - np.min(X[np.nonzero(X[:, i]), i])) / (\n",
        "                        np.max(X[np.nonzero(X[:, i]), i]) - np.min(X[np.nonzero(X[:, i]), i]))\n",
        "        return X\n",
        "\n",
        "    def reading(self, address):\n",
        "        train_folder = address + 'TRAIN_FEATURES/'\n",
        "        mydir = train_folder\n",
        "        test_folder = address + 'TEST_FEATURES/'\n",
        "        files = os.listdir(train_folder)\n",
        "        labels = list()\n",
        "        mydir2 = test_folder\n",
        "        global files3\n",
        "        try:\n",
        "            files3 = os.listdir(mydir2)\n",
        "\n",
        "\n",
        "        except:\n",
        "            files3=[]\n",
        "        cont2 = 0\n",
        "\n",
        "        # number of species\n",
        "        cont3 = 0\n",
        "        x_train = np.zeros((133, self.alfa * self.dim), dtype=np.float)\n",
        "        y_train = np.zeros((self.alfa * self.dim, self.alfa), dtype=np.int)\n",
        "\n",
        "        x_test = np.zeros((133, self.alfa * self.dim2), dtype=np.float)\n",
        "        y_test = np.zeros((self.alfa * self.dim2, self.alfa), dtype=np.int)\n",
        "\n",
        "        target = np.zeros((self.alfa * self.dim), dtype=np.int)\n",
        "        species = np.zeros((self.alfa * self.dim), dtype=np.str)\n",
        "\n",
        "        cont = 0\n",
        "        # reading training data\n",
        "        for j in range(0, len(files)):\n",
        "\n",
        "            if files[j] != \".DS_Store\":\n",
        "\n",
        "                files_TRAIN = os.listdir(mydir + '/' + files[j])\n",
        "\n",
        "                labels.append(files[j])\n",
        "                aa = 0\n",
        "                # flag for reading\n",
        "\n",
        "                for i in range(0, len(files_TRAIN)):\n",
        "\n",
        "                    if files_TRAIN[i] != \".DS_Store\" and aa < self.dim:\n",
        "\n",
        "                        file_read = open(mydir + files[j] + '/' + files_TRAIN[aa], 'r')\n",
        "\n",
        "                        try:\n",
        "\n",
        "                            x_train[:, cont2] = np.loadtxt(file_read)\n",
        "\n",
        "                            y_train[cont2, cont] = 1\n",
        "                            cont2 += 1\n",
        "                            aa += 1\n",
        "                            target[cont2] = j\n",
        "                            species[cont2] = files[j]\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                cont += 1\n",
        "        # reading test data\n",
        "        cont = 0\n",
        "\n",
        "        if self.static==0:\n",
        "\n",
        "            for j in range(0, len(files)):\n",
        "\n",
        "                if files[j] != \".DS_Store\":\n",
        "\n",
        "                    files_TEST = os.listdir(mydir2 + '/' + files[j])\n",
        "                    # flag for reading\n",
        "                    aa = 0\n",
        "\n",
        "                    for i in range(0, len(files_TEST)):\n",
        "\n",
        "                        if files_TEST[i] != \".DS_Store\" and aa < self.dim2:\n",
        "\n",
        "                            file_read = open(mydir2 + files[j] + '/' + files_TEST[aa], 'r')\n",
        "\n",
        "                            try:\n",
        "\n",
        "                                x_test[:, cont3] = np.loadtxt(file_read)\n",
        "                                y_test[cont3, cont] = 1\n",
        "                                cont3 += 1\n",
        "                                aa += 1\n",
        "                            except:\n",
        "                                pass\n",
        "                    cont += 1\n",
        "\n",
        "            x_train = np.transpose(x_train)\n",
        "\n",
        "            x_test = np.transpose(x_test)\n",
        "\n",
        "            x_train = np.delete(x_train, 122, axis=1)\n",
        "\n",
        "            x_test = np.delete(x_test, 122, axis=1)\n",
        "\n",
        "            x_train = x_train[:, :-1]\n",
        "            x_test = x_test[:, :-1]\n",
        "\n",
        "        else:\n",
        "            x_train = np.transpose(x_train)\n",
        "            x_train = np.delete(x_train, 122, axis=1)\n",
        "\n",
        "            x_train = x_train[:, :-1]\n",
        "\n",
        "\n",
        "        import copy\n",
        "\n",
        "        y_train = y_train[0:self.alfa * self.dim, 0:self.alfa]\n",
        "        y_test = y_test[0:self.alfa * self.dim2, 0:self.alfa]\n",
        "\n",
        "        np.savetxt(self.address + 'x_train_for_further_code.csv', x_train,\n",
        "                   delimiter=\",\")\n",
        "        np.savetxt(self.address + 'x_test_for_further_code.csv', x_test,\n",
        "                   delimiter=\",\")\n",
        "\n",
        "        return x_train, y_train, x_test, y_test, labels\n",
        "\n",
        "    def evaluate_purity(self, k, predicted, Y, size):\n",
        "\n",
        "        score_best = np.zeros((k, k))\n",
        "\n",
        "        for i in range(0, k):\n",
        "            for j in range(0, k):\n",
        "                # computing all the possible overlap\n",
        "                score_best[i, j] = np.count_nonzero(Y[predicted[i], j] == 1)\n",
        "\n",
        "        # taking the maximum arguments and values for accuracy\n",
        "        master = np.argmax(score_best, axis=0)\n",
        "        records_array = master\n",
        "        idx_sort = np.argsort(records_array)\n",
        "        sorted_records_array = records_array[idx_sort]\n",
        "        vals, idx_start, count = np.unique(sorted_records_array, return_counts=True,\n",
        "                                           return_index=True)\n",
        "\n",
        "        # sets of indices\n",
        "\n",
        "        res = np.split(idx_sort, idx_start[1:])\n",
        "\n",
        "        # filter them with respect to their size, keeping only items occurring more than once\n",
        "\n",
        "        vals = vals[count > 1]\n",
        "        a = vals\n",
        "        a = np.unique(a)\n",
        "\n",
        "        specie_over = list()\n",
        "\n",
        "        for j in range(0, len(a)):\n",
        "            specie_over.append(np.where(master == a[j]))\n",
        "        maxs = np.max(score_best, axis=1)\n",
        "\n",
        "        # maxs = np.max(score_best, axis=0)\n",
        "        master2 = np.argmax(score_best, axis=1)\n",
        "\n",
        "        purit = np.sum(maxs) / size\n",
        "        return master2, purit, score_best, specie_over\n",
        "\n",
        "    def evaluate_purity_OVERLAP(self, k, predicted, Y, size):\n",
        "\n",
        "        score_best = np.zeros((k, k))\n",
        "        for i in range(0, k):\n",
        "            for j in range(0, k):\n",
        "                # computing all the possible overlap\n",
        "                score_best[i, j] = np.count_nonzero(Y[predicted[i], j] == 1)\n",
        "        # taking the maximum arguments and values for accuracy\n",
        "        master = np.argmax(score_best, axis=0)\n",
        "        records_array = master\n",
        "        idx_sort = np.argsort(records_array)\n",
        "        sorted_records_array = records_array[idx_sort]\n",
        "        vals, idx_start, count = np.unique(sorted_records_array, return_counts=True,\n",
        "                                           return_index=True)\n",
        "        # sets of indices\n",
        "        res = np.split(idx_sort, idx_start[1:])\n",
        "        # filter them with respect to their size, keeping only items occurring more than once\n",
        "        specie_over = list()\n",
        "        vals = vals[count > 1]\n",
        "        a = vals\n",
        "        a = np.unique(a)\n",
        "        master2 = np.argmax(score_best, axis=1)\n",
        "\n",
        "        for i in range(0, len(a)):\n",
        "            specie_over.append(np.where(master == a[i])[0])\n",
        "        # this is referred to species\n",
        "        maxs = np.max(score_best, axis=0)\n",
        "        # this is referred to clusters\n",
        "        maxs2 = np.max(score_best, axis=1)\n",
        "        cluster_totals = np.asarray(list(range(0, self.alfa)))\n",
        "        macrocluster = list()\n",
        "        species_overlapped = list()\n",
        "\n",
        "        self.num_species = self.alfa - len(specie_over)\n",
        "\n",
        "        for i in range(0, len(cluster_totals)):\n",
        "            if not i in master2:\n",
        "                maxs2[master[i]] += maxs2[master[i]]\n",
        "                macrocluster.append(master[i])\n",
        "                species_overlapped.append(i)\n",
        "            # maxs = np.max(score_best, axis=0)\n",
        "        purit = np.sum(maxs2) / size\n",
        "        return master2, purit, score_best, specie_over, macrocluster, species_overlapped\n",
        "\n",
        "    def isdifferente(self, x111):\n",
        "        elements = np.zeros((len(x111), 1)) - 1\n",
        "        num = 0\n",
        "        for i in range(0, len(x111)):\n",
        "            if elements[x111[i]] == -1:\n",
        "                elements[x111[i]] = i\n",
        "            else:\n",
        "                num += 1\n",
        "        return num\n",
        "\n",
        "    def PCA_custom(self, x_def, y_def):\n",
        "\n",
        "        #####################\n",
        "\n",
        "        if self.static ==1:\n",
        "            x_def = np.column_stack((x_def[:,0:118],x_def[:,122:]))\n",
        "        ##################################### DELETE!\n",
        "        from sklearn import preprocessing\n",
        "        from sklearn.decomposition import PCA\n",
        "        # x_def, y_def = self.binning(5,x_def,y_def)\n",
        "        std_scale = preprocessing.StandardScaler().fit(x_def)\n",
        "        x_def = std_scale.transform(x_def)\n",
        "\n",
        "        # PCA from scikit-learn\n",
        "        pca = PCA(n_components=np.shape(x_def)[1])\n",
        "        x_def[np.isnan(x_def)] = 0\n",
        "        x_def[np.isinf(x_def)] = 0\n",
        "\n",
        "        pca.fit(x_def)\n",
        "        X = pca.transform(x_def)\n",
        "        files = self.files\n",
        "        labels = files\n",
        "\n",
        "        eigenvectors = pca.components_\n",
        "        var = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3) * 100)\n",
        "        plt.plot(var)\n",
        "        plt.style.context('seaborn-whitegrid')\n",
        "        plt.ylabel('% Variance Explained')\n",
        "        plt.xlabel('# of Features')\n",
        "        plt.title('PCA Analysis')\n",
        "        plt.ylim(30, 100.5)\n",
        "        num_species = self.alfa\n",
        "        from mpl_toolkits.mplot3d import Axes3D\n",
        "        from sklearn import preprocessing\n",
        "\n",
        "        # this is beacuse we want to use MYC+RAS as testing data, thus, I fix the trains, but only for the NN not for the PCA\n",
        "        y = y_def\n",
        "        z = np.zeros((len(y), 1))\n",
        "\n",
        "        for i in range(0, num_species):\n",
        "            z[y[:, i] == 1, :]\n",
        "\n",
        "        jet = plt.cm.jet\n",
        "        colors = jet(np.linspace(0, 1, self.alfa))\n",
        "        c = colors\n",
        "        plt.figure()\n",
        "\n",
        "        for i in range(0, num_species):\n",
        "            plt.scatter(X[y[:, i] == 1, :][:, 0], X[y[:, i] == 1, :][:, 1], c=c[i], edgecolor='k', label=labels[i])\n",
        "\n",
        "        plt.legend()\n",
        "        plt.ylabel('PC1')\n",
        "        plt.xlabel('PC0')\n",
        "        plt.figure()\n",
        "        for i in range(0, num_species):\n",
        "            plt.scatter(X[y[:, i] == 1, :][:, 0], X[y[:, i] == 1, :][:, 2], c=c[i], edgecolor='k', label=labels[i])\n",
        "\n",
        "        plt.legend()\n",
        "        plt.ylabel('PC0')\n",
        "        plt.xlabel('PC2')\n",
        "        plt.figure()\n",
        "\n",
        "        for i in range(0, num_species):\n",
        "            plt.scatter(X[y[:, i] == 1, :][:, 1], X[y[:, i] == 1, :][:, 2], c=c[i], edgecolor='k', label=labels[i])\n",
        "        plt.ylabel('PC1')\n",
        "        plt.xlabel('PC2')\n",
        "\n",
        "        plt.legend()\n",
        "        fig = plt.figure()\n",
        "        ax = Axes3D(fig)\n",
        "\n",
        "        for i in range(0, num_species):\n",
        "            ax.scatter(X[y[:, i] == 1, :][:, 0], X[y[:, i] == 1, :][:, 1], X[y[:, i] == 1, :][:, 2], c=c[i],\n",
        "                       edgecolor='k', label=labels[i])\n",
        "        ax.w_xaxis.set_ticklabels(['PC 1'])\n",
        "        ax.w_yaxis.set_ticklabels(['PC 2'])\n",
        "        ax.w_zaxis.set_ticklabels(['PC 3'])\n",
        "        ax.legend(ncol=2)\n",
        "\n",
        "        plt.show()\n",
        "        if self.static==0:\n",
        "            self.m = 1.6\n",
        "        else:\n",
        "            self.m = 1.25\n",
        "\n",
        "        return X, eigenvectors\n",
        "\n",
        "    def clusters_comp(self, Train_original):\n",
        "        import statistics\n",
        "\n",
        "        maxssss3 = list()\n",
        "\n",
        "        for t in range(0, 10):\n",
        "            X2 = copy.copy(Train_original)\n",
        "            X2 = X2[~np.all(X2 == 0, axis=1)]\n",
        "            X2[np.isnan(X2)] = 0\n",
        "            X2[np.isinf(X2)] = 0\n",
        "            X2 = self.normalize(X2)\n",
        "            X2[np.isnan(X2)] = 0\n",
        "            X2[np.isinf(X2)] = 0\n",
        "            partition_entropy = list()\n",
        "\n",
        "            for azz in range(2, 12):\n",
        "                X3 = copy.copy(X2)\n",
        "                k = azz\n",
        "                fuzzy_kmeans = FuzzyKMeans(k=azz, m=1.4)\n",
        "                fuzzy_kmeans.fit(X3)\n",
        "                #PARTITION ENTROPY DEFINITION\n",
        "                F2 = 1 / np.shape(fuzzy_kmeans.fuzzy_labels_)[0] * np.sum(\n",
        "                    (np.sum(fuzzy_kmeans.fuzzy_labels_ * np.log(fuzzy_kmeans.fuzzy_labels_), axis=1)))\n",
        "                partition_entropy.append((F2 - 1 / azz) / (1 - 1 / azz))\n",
        "            maxssss3.append(np.argmax(np.asarray(partition_entropy)) + 2)\n",
        "\n",
        "        try:\n",
        "            print('num_clusters = ' + str(statistics.mode(partition_entropy)))\n",
        "        except:\n",
        "            if len(np.unique(partition_entropy))==0:\n",
        "                #very robust estimation of number of clusters\n",
        "                print('num_clusters = ' + str(partition_entropy[0]))\n",
        "\n",
        "    def unsupervised_partitioning(self, x_train, y_train, X):\n",
        "        import random\n",
        "\n",
        "        Train_original = copy.copy(x_train)\n",
        "\n",
        "\n",
        "        X2 = copy.copy(Train_original)\n",
        "\n",
        "        X3 = copy.copy(X2)\n",
        "\n",
        "\n",
        "        dim_Train = self.dim_Train\n",
        "        dim_Test = self.dim - self.dim_Train\n",
        "        X_train = np.zeros((dim_Train * self.alfa, 131))\n",
        "        Y_train = np.zeros((dim_Train * self.alfa, self.alfa))\n",
        "        for i in range(0, self.alfa):\n",
        "            X_train[i * dim_Train:i * dim_Train + dim_Train, :] = X3[i * self.dim:i * self.dim + dim_Train, :]\n",
        "            Y_train[i * dim_Train:i * dim_Train + dim_Train, :] = y_train[i * self.dim:i * self.dim + dim_Train, :]\n",
        "\n",
        "        self.clusters_comp(X_train)\n",
        "\n",
        "        self.X7 = copy.copy(X_train)\n",
        "        X3 = copy.copy(X_train)\n",
        "        self.y_train = y_train\n",
        "\n",
        "        y_train = copy.copy(Y_train)\n",
        "\n",
        "\n",
        "        fuzzy_kmeans = FuzzyKMeans(k=self.alfa, m=self.m)\n",
        "        X2 = copy.copy(X3)\n",
        "        # X2 = X2[:,0:120]\n",
        "        if self.static==1:\n",
        "            X2 = np.column_stack((X2[:,0:118],X2[:,122:]))\n",
        "\n",
        "        X2[np.isnan(X2)] = 0\n",
        "\n",
        "        X2[np.isinf(X2)] = 0\n",
        "\n",
        "        X2 = X2[~np.all(X2 == 0, axis=1)]\n",
        "        X2 = self.normalize(X2)\n",
        "        X2[np.isnan(X2)] = 0\n",
        "\n",
        "        X2[np.isinf(X2)] = 0\n",
        "\n",
        "        # X2 = X2[~np.all(X2 == 0, axis=1)]\n",
        "\n",
        "        try:\n",
        "            fuzzy_kmeans.fit(X2)\n",
        "        except:\n",
        "            pass\n",
        "        num_species = self.alfa\n",
        "        k = self.alfa\n",
        "        labels = list()\n",
        "        for i in range(0, self.alfa):\n",
        "            labels.append('cluster' + str(i))\n",
        "        predicted = list()\n",
        "        for p in range(0, k):\n",
        "            predicted.append(list())\n",
        "        for i in range(0, k):\n",
        "            predicted[i].append(np.where(fuzzy_kmeans.labels_ == i))\n",
        "        master, purit, scorororo, spe, macro, r3 = self.evaluate_purity_OVERLAP(num_species, predicted, y_train,\n",
        "                                                                                np.shape(y_train)[0])\n",
        "\n",
        "        fig2 = plt.figure()\n",
        "        ax = fig2.add_subplot(111, projection='3d')\n",
        "        jet = plt.cm.jet\n",
        "        colors = jet(np.linspace(0, 1, self.alfa))\n",
        "        colorss = colors\n",
        "\n",
        "        for i in range(0, k):\n",
        "            ax.scatter(X[predicted[i], 0], X[predicted[i], 1], X[predicted[i], 2], c=colorss[master[i]], edgecolor='k',\n",
        "                       label=labels[master[i]])\n",
        "        plt.legend(ncol=2)\n",
        "        ax.w_xaxis.set_ticklabels(['PC 1'])\n",
        "        ax.w_yaxis.set_ticklabels(['PC 2'])\n",
        "        ax.w_zaxis.set_ticklabels(['PC 3'])\n",
        "        plt.figure()\n",
        "        for i in range(0, num_species):\n",
        "            plt.scatter(X[predicted[i], 0], X[predicted[i], 1], c=colorss[master[i]], edgecolor='k', label=labels[i])\n",
        "        plt.legend()\n",
        "        plt.figure()\n",
        "        plt.ylabel('PC0')\n",
        "        plt.xlabel('PC1')\n",
        "        for i in range(0, num_species):\n",
        "            plt.scatter(X[predicted[i], 1], X[predicted[i], 2], c=colorss[master[i]], edgecolor='k', label=labels[i])\n",
        "        plt.legend()\n",
        "        plt.ylabel('PC1')\n",
        "        plt.xlabel('PC2')\n",
        "        plt.figure()\n",
        "        for i in range(0, num_species):\n",
        "            plt.scatter(X[predicted[i], 0], X[predicted[i], 2], c=colorss[master[i]], edgecolor='k', label=labels[i])\n",
        "        plt.legend()\n",
        "        plt.ylabel('PC0')\n",
        "        plt.xlabel('PC2')\n",
        "        plt.legend()\n",
        "        print(purit)\n",
        "        print(self.isdifferente(master))\n",
        "        plt.show()\n",
        "        centroid = fuzzy_kmeans.cluster_centers_\n",
        "\n",
        "        labels_true_for_Test = np.zeros((self.alfa * dim_Train, k))\n",
        "\n",
        "        for aaa in range(0, self.alfa):\n",
        "            labels_true_for_Test[predicted[aaa], master[aaa]] = 1\n",
        "\n",
        "        y_train4 = copy.copy(self.y_train)\n",
        "\n",
        "        for a in range(0, len(macro)):\n",
        "            y_train4[np.where(y_train4[:, r3[a]] == 1)[0], macro[a]] = 1\n",
        "            y_train4[np.where(y_train4[:, r3[a]] == 1)[0], r3[a]] = 0\n",
        "\n",
        "        y_train4 = np.transpose(y_train4)\n",
        "        labels_true_for_Test = np.transpose(labels_true_for_Test)\n",
        "\n",
        "        labels_true_for_Test = labels_true_for_Test[~np.all(y_train4 == 0, axis=1)]\n",
        "\n",
        "        y_train4 = y_train4[~np.all(y_train4 == 0, axis=1)]\n",
        "        y_train4 = np.transpose(y_train4)\n",
        "        labels_true_for_Test = np.transpose(labels_true_for_Test)\n",
        "\n",
        "        self.y_train4 = y_train4\n",
        "\n",
        "        return labels_true_for_Test\n",
        "\n",
        "    def GMM(self, X2, X, y_train):\n",
        "        from sklearn.mixture import GaussianMixture\n",
        "        k = self.alfa\n",
        "        X2 = normalize(X2)\n",
        "        gmm = GaussianMixture(n_components=self.alfa)\n",
        "        gmm.fit(X2)\n",
        "        predicted_gmm = gmm.predict(X2)\n",
        "        fig4 = plt.figure()\n",
        "        ax = fig4.add_subplot(111, projection='3d')\n",
        "        jet = plt.cm.jet\n",
        "        labels = ['cluster 1', 'cluster 2', 'cluster 3', 'cluster 4', 'cluster 5', 'cluster 6', 'cluster 7', 'cluster 8',\n",
        "                  'cluster 9', 'cluster 10', 'cluster11', 'cluster12', 'cluster13', 'cluster14', 'cluster15', 'cluster 16',\n",
        "                  'cluster 17', 'cluster 18', 'cluster19', 'cluster20', 'cluster21', 'cluster22']\n",
        "\n",
        "        colorss = jet(np.linspace(0, 1, self.alfa))\n",
        "        predicted1 = list()\n",
        "        for p in range(0, k):\n",
        "            predicted1.append(list())\n",
        "        for i in range(0, k):\n",
        "            predicted1[i].append(np.where(predicted_gmm == i))\n",
        "        master, purit = self.evaluate_purity(k, predicted1, y_train, np.shape(X2)[0])\n",
        "\n",
        "        for i in range(0, k):\n",
        "            ax.scatter(X[predicted1[i], 0], X[predicted1[i], 1], X[predicted1[i], 2], c=colorss[master[i]], edgecolor='k',\n",
        "                       label=labels[master[i]])\n",
        "        plt.legend()\n",
        "        ax.w_xaxis.set_ticklabels(['PC 1'])\n",
        "        ax.w_yaxis.set_ticklabels(['PC 2'])\n",
        "        ax.w_zaxis.set_ticklabels(['PC 3'])\n",
        "        print(purit)\n",
        "        print(self.isdifferente(master))\n",
        "        return gmm\n",
        "\n",
        "    def oneclassSVM(self, X2, y_train, x_test, y_test, address):\n",
        "        accuracy = np.zeros((2, 10))\n",
        "\n",
        "        from sklearn import svm\n",
        "\n",
        "        for c in range(0, 10):\n",
        "\n",
        "            x = X2[np.where(y_train[:, c] == 1), :][0]\n",
        "            maxs = np.max(x, axis=0)\n",
        "            mins = np.min(x, axis=0)\n",
        "            x_t = copy.copy(x_test)\n",
        "            if c != 9:\n",
        "                x = np.row_stack((x))\n",
        "            else:\n",
        "                x = np.row_stack((x))\n",
        "\n",
        "            for i in range(0, 131):\n",
        "                x_t[:, i] = (x_t[:, i] - mins[i]) / (maxs[i] - mins[i])\n",
        "\n",
        "                x[:, i] = (x[:, i] - mins[i]) / (maxs[i] - mins[i])\n",
        "\n",
        "            clf = svm.OneClassSVM(kernel='rbf', degree=5, nu=0.1)\n",
        "            clf.fit(x)\n",
        "\n",
        "            a = clf.predict(x_t[c * 140:c * 140 + 140, :])\n",
        "            print(len(np.where(a == -1)[0]))\n",
        "            accuracy[0, c] = 140 - len(np.where(a == -1)[0])\n",
        "            a = clf.predict(np.row_stack((x_t[0:c * 140, :], x_t[c * 140 + 140:, :])))\n",
        "            print(len(np.where(a == -1)[0]))\n",
        "            accuracy[1, c] = len(np.where(a == -1)[0])\n",
        "            np.savetxt(address + 'accurac1class.csv', accuracy, delimiter=',')\n",
        "\n",
        "    def randomforest(self, X, labels_true_for_Test, x_test, y_test):\n",
        "        from sklearn.ensemble import RandomForestRegressor\n",
        "        from sklearn.metrics import r2_score\n",
        "        from scipy.stats import spearmanr, pearsonr\n",
        "\n",
        "        rf = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=15)\n",
        "\n",
        "        rf.fit(X, labels_true_for_Test)\n",
        "        predicted_test = rf.predict(x_test)\n",
        "        predicted_binary = np.argmax(predicted_test, axis=1)\n",
        "        predic = np.zeros_like(predicted_test)\n",
        "        for i in range(0, len(predicted_binary)):\n",
        "            predic[i, int(predicted_binary[i])] = 1\n",
        "        test_score = r2_score(y_test, predicted_test)\n",
        "        accuracy = 0\n",
        "        for i in range(0, len(y_test)):\n",
        "            if y_test[i, int(predicted_binary[i])] == 1:\n",
        "                accuracy += 1\n",
        "        print(accuracy / len(np.where(y_test != 0)[0]))\n",
        "\n",
        "    def neuralnet_for_classification(self, x_train, y_train, x_test, y_test, features, alfa, dim, dim2, address, nepoch):\n",
        "\n",
        "        # network model already available\n",
        "        model = keras.models.load_model(address + 'myNetunsupervised.h5')\n",
        "        y_pred_val = model.predict(x_test)\n",
        "        accuracy = ((np.shape(np.nonzero(np.argmax(y_pred_val,axis=1) - np.argmax(y_test,axis=1))))[1])/np.shape(x_test)[0]\n",
        "        print('neural_netacc = ' + str(accuracy))\n",
        "\n",
        "    def DEC_test_and_newspeciescomputation(self, x_testT, files, address):\n",
        "        predicted = np.zeros((140, 10, 10))\n",
        "        MOC = np.zeros((10, 1))\n",
        "        MOC2 = np.zeros((10, 1))\n",
        "\n",
        "        import copy\n",
        "\n",
        "        for j in range(0, 10):\n",
        "            out_of_class = 0\n",
        "            model = keras.models.load_model(address + '/TRAINED DETECTORS/' + files[j] + '.h5')\n",
        "            x_testz = copy.copy(x_testT)\n",
        "            x_test2 = self.normalize_test_train_for_newclasses(131, x_testz, files[j])\n",
        "            count = 0\n",
        "            anom = 0\n",
        "            for species in range(0, 10):\n",
        "                if j != species:\n",
        "                    out_of_class += np.sum(np.argmax(model.predict(x_test2[species * 140:species * 140 + 140, :]), axis=1))\n",
        "                    predicted[:, species, j] = np.argmax(model.predict(x_test2[species * 140:species * 140 + 140, :]),\n",
        "                                                         axis=1)\n",
        "                    count += 1\n",
        "                    anom += np.sum((np.argmax(model.predict(x_test2[species * 140:species * 140 + 140, :]), axis=1)))\n",
        "                if j == species:\n",
        "                    classin = np.sum(np.argmax(model.predict(x_test2[species * 140:species * 140 + 140, :]), axis=1))\n",
        "                    print('classification accuracy' + files[j] + '=' + str(140 - classin))\n",
        "            print('anomaly detection accuracy' + files[j] + '=' + str(anom))\n",
        "            in_class = np.shape(np.argmax(model.predict(x_test2[j * 140:j * 140 + 140, :]), axis=1))\n",
        "            MOC[j] = out_of_class\n",
        "            MOC2[j] = in_class\n",
        "\n",
        "        print(np.sum(np.prod(predicted[:, 0, 1:], axis=1)))\n",
        "\n",
        "        for j in range(1, 9):\n",
        "            print(np.sum(np.prod(np.column_stack((predicted[:, j, 0:j], predicted[:, j, j + 1:])), axis=1)))\n",
        "\n",
        "        print(np.sum(np.prod(predicted[:, 9, :-1], axis=1)))\n",
        "\n",
        "    def __init__(self, address, image_segmentation_processing=0, feature_recomputing=0, unsupervised_partitioning=1, classification=1,\n",
        "                 DEC_testing=1, oneclassSVM=0, robustness_test=0):\n",
        "\n",
        "        if 'LENSLESS' in address:\n",
        "            self.static = 0\n",
        "        else:\n",
        "            self.static = 1\n",
        "\n",
        "        if self.static==1:\n",
        "            self.static = 1\n",
        "            self.dim = 90\n",
        "            self.dim_Train = 75\n",
        "            DEC_testing = 0\n",
        "            oneclassSVM = 0\n",
        "\n",
        "        else:\n",
        "            self.static = 0\n",
        "            self.dim = 500\n",
        "            self.dim_Train = self.dim\n",
        "\n",
        "\n",
        "        try:\n",
        "            self.files = os.listdir(address + 'TRAIN_FEATURES/')\n",
        "\n",
        "            for i in range(0, len(self.files)):\n",
        "                self.files[i] = self.files[i].upper()\n",
        "\n",
        "\n",
        "        except:\n",
        "            self.files = os.listdir(address + 'TRAIN_IMAGE/')\n",
        "\n",
        "        self.address = address\n",
        "        self.alfa = len(self.files)\n",
        "        # size of training data\n",
        "\n",
        "\n",
        "        # size of testing data\n",
        "        self.dim2 = 140\n",
        "\n",
        "        # we read the files resulting from the IMAGE PROCESSOR FOR THE PLANKTON CLASSIFIER\n",
        "        #\n",
        "\n",
        "        if image_segmentation_processing == 1:\n",
        "            self.image_processor(address)\n",
        "\n",
        "        if feature_recomputing == 1:\n",
        "\n",
        "            self.feature_extractor(address=address, train_folder=address + 'TRAIN_IMAGE/',\n",
        "                                   train_folder_bin=address + 'BIN_TRAIN_IMAGE/', files=self.files)\n",
        "\n",
        "        x_train, y_train, x_test, y_test, labels = self.reading(address)\n",
        "\n",
        "        self.X3 = copy.copy(x_train)\n",
        "        self.X4 = copy.copy(x_train)\n",
        "        self.X5 = copy.copy(x_train)\n",
        "        self.X6 = copy.copy(x_train)\n",
        "\n",
        "        # number of extracted features\n",
        "        self.features = x_train.shape[1]\n",
        "        # let us visualize our space using the PCA\n",
        "\n",
        "        if unsupervised_partitioning == 1:\n",
        "            # self.class_imbalance_test(self.address + 'TRAIN_FEATURES\\\\',y_train)\n",
        "            # unsupervised partitioning of plankton classifier, returning our labels\n",
        "            X, components = self.PCA_custom(x_train, y_train)\n",
        "            self.labels_true_for_Test = self.unsupervised_partitioning(x_train, y_train, X)\n",
        "            self.x_test_NN = copy.copy(x_test)\n",
        "            self.x_test_DEC = copy.copy(x_test)\n",
        "            #### mixture of gaussians\n",
        "            # gmm = GMM(X2,X,y_train)\n",
        "\n",
        "        if classification == 1:\n",
        "\n",
        "            if unsupervised_partitioning == 0:\n",
        "                X, components = self.PCA_custom(x_train, y_train)\n",
        "                self.labels_true_for_Test = self.unsupervised_partitioning(x_train, y_train, X)\n",
        "                self.x_test_NN = copy.copy(x_test)\n",
        "                self.x_test_DEC = copy.copy(x_test)\n",
        "            ### classification with neural network\n",
        "\n",
        "            self.X4[np.isnan(self.X4)] = 0\n",
        "\n",
        "            self.X4[np.isinf(self.X4)] = 0\n",
        "\n",
        "            self.X4[np.isnan(self.X4)] = 0\n",
        "\n",
        "            self.X4[np.isinf(self.X4)] = 0\n",
        "\n",
        "            from sklearn import model_selection\n",
        "\n",
        "\n",
        "            if self.static==1:\n",
        "                dim_Train = self.dim_Train\n",
        "                X_train = np.zeros((self.dim_Train * self.alfa, 131))\n",
        "                # Y_train = np.zeros(( self.dim_Train*self.alfa,self.alfa))\n",
        "\n",
        "                # X_train = self.X6\n",
        "\n",
        "                y_train = self.y_train4\n",
        "\n",
        "                dim_Test = self.dim - self.dim_Train\n",
        "                X_test = np.zeros(((dim_Test * self.alfa), 131))\n",
        "                Y_test = np.zeros(((dim_Test * self.alfa), np.shape(y_train)[1]))\n",
        "\n",
        "                for i in range(0, np.shape(y_train)[1]):\n",
        "                    # dim_Train = int(num_*0.7)\n",
        "\n",
        "                    X_train[i * dim_Train:i * dim_Train + dim_Train, :] = self.X4[i * self.dim:i * self.dim + dim_Train, :]\n",
        "                    # Y_train[i * dim_Train:i * dim_Train + dim_Train, :] =     y_train[i*self.dim:i*self.dim+dim_Train,:]\n",
        "\n",
        "                    X_test[i * dim_Test:i * dim_Test + dim_Test, :] = self.X4[\n",
        "                                                                      i * self.dim + self.dim_Train:i * self.dim + self.dim, :]\n",
        "                    Y_test[i * dim_Test:i * dim_Test + dim_Test, :] = y_train[\n",
        "                                                                      i * self.dim + self.dim_Train:i * self.dim + self.dim, :]\n",
        "\n",
        "                X_train, X_test = self.normalize_test_train(self.features, X_train, X_test)\n",
        "\n",
        "                X_train = np.column_stack((X_train[:, 0:118], X_train[:, 122:]))\n",
        "                X_test = np.column_stack((X_test[:, 0:118], X_test[:, 122:]))\n",
        "\n",
        "                self.randomforest(X_train, self.labels_true_for_Test, X_test, Y_test)\n",
        "\n",
        "            else:\n",
        "                X_train, X_test = self.normalize_test_train(self.features, x_train, x_test)\n",
        "                self.neuralnet_for_classification(X_train, self.labels_true_for_Test, X_test, y_test, self.features,\n",
        "                                                  self.alfa, self.dim,\n",
        "                                                  self.dim2, address, 250)\n",
        "            ##########classification with random forest\n",
        "\n",
        "        elif classification == 2:\n",
        "            if unsupervised_partitioning == 0:\n",
        "                X, components = self.PCA_custom(x_train, y_train)\n",
        "                self.labels_true_for_Test = self.unsupervised_partitioning(x_train, y_train, X)\n",
        "                self.x_test_NN = copy.copy(x_test)\n",
        "                self.x_test_DEC = copy.copy(x_test)\n",
        "            self.X3, x_test_NN = self.normalize_test_train(self.features, self.X3, self.x_test_NN)\n",
        "            self.randomforest(self.X3, self.labels_true_for_Test, self.x_test_NN, y_test)\n",
        "\n",
        "        if DEC_testing == 1:\n",
        "            # anomaly detector of plankton classifier\n",
        "            if unsupervised_partitioning == 0:\n",
        "                X, components = self.PCA_custom(x_train, y_train)\n",
        "                self.labels_true_for_Test = self.unsupervised_partitioning(x_train, y_train, X)\n",
        "                self.x_test_NN = copy.copy(x_test)\n",
        "                self.x_test_DEC = copy.copy(x_test)\n",
        "            self.DEC_test_and_newspeciescomputation(self.x_test_DEC, self.files, address)\n",
        "\n",
        "        if oneclassSVM == 1:\n",
        "            # anomaly detector based on one class SVM\n",
        "            if unsupervised_partitioning == 0:\n",
        "                X, components = self.PCA_custom(x_train, y_train)\n",
        "                self.labels_true_for_Test = self.unsupervised_partitioning(x_train, y_train, X)\n",
        "                self.x_test_NN = copy.copy(x_test)\n",
        "                self.x_test_DEC = copy.copy(x_test)\n",
        "            self.oneclassSVM(self.X5, self.labels_true_for_Test, self.x_test_DEC, y_test, address)\n",
        "        if robustness_test == 1:\n",
        "            self.robustness_test(self.X5, y_train, self.features, address, self.files)\n",
        "\n",
        "####### Insert here the address for the PLANKTON DATASET\n",
        "address = '' #### change this line with actual address\n",
        "\n",
        "Test = PLANKTON_CLASSIFIER(address=address, image_segmentation_processing=0, feature_recomputing=0,\n",
        "                           unsupervised_partitioning=1, classification=1, DEC_testing=1, oneclassSVM=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}